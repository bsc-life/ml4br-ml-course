# ml4br-ml-course

## 1. Course content
### 1.1 Day 1 – Data Processing and Feature Engineering

- Python Refresher (pandas, numpy, plotting, scikit-learn) 
- Data Exploration 
- Data Preprocessing 
- Feature Engineering (Extraction & Selection)
- Hands-on session 

### 1.2. Day 2 – Supervised and Unsupervised learning

Supervised Learning
- Linear Regression, Logistic Regression, Tree-Based Methods
- Random Forest, XGBoost
Unsupervised Learning (1 h)
- K-means, DBSCAN
- Hands-on: Training and Testing Supervised and Unsupervised Models

We are going to use Google Collab, so please make sure you have a google account on hand. You can also run Jupyter on your local system, but we will not do this during the lesson! (See https://jupyter.org/install)
Attendance to Day 1 is highly encouraged to fully take advantage of Day 2.

### 1.3. Day 3 – Deep Learning
Part I: The world of Deep Learning
- Differences between Machine Learning and Deep Learning
- Backpropagation algorithm
- Deep Dive into Convolutional Neural Networks (CNNs)
- Deep Learning frameworks: PyTorch and fast.ai
- Hands-on overview of the DL applications: image classification, image segmentation, text classification, tabular data, collaborative filtering (recommendation systems)
- SHAP values for explainability

Part II: Neural Networks Dive-In
- A Neural Network at Work
- How do Neural Networks Learn?
- Limitations
- Introduction to Pytorch:
- A Basic Neural Network
- Binary Classification of Tabular Data
- Binary Classification of Images

Requirements (Optional, if we have time for that…):
- Register for a free version of ChatGPT (or alternatives like Claude, DeepSeek, etc.) to cover LLMs prompting for software development
- If you want to run an image classifier on your data, upload images to your Google Drive. Preferred structure of the dataset - each image class sits in its own folder.

### 1.4. Day 4 – Large Language Models from Theory to Applications
Throughout the four hours, we’ll explore the core ideas behind Large Language Models (LLMs), breaking down the foundational principles of the Transformer architecture and the attention mechanism that powers them. Along the way, we’ll examine both the strengths and limitations of these models, including the challenges of scaling them to real-world applications. We will conclude the day with a demo of the hugging face transformers library and a practical exercise of fine-tuning a large language model on a collection of life-sciences inspired NLP problems.


## 2. Links to Google Colabs:
### 2.1. Day 1

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bsc-life/ml4br-ml-course/blob/main/nbs/day_1/Introduction%20to%20Machine%20Learning%20for%20Life%20Sciences%20-%20Day%201%3A%20Data%20Processing%20and%20Feature%20Engineering%20.ipynb)

### 2.2. Day 2

### 2.3. Day 3

### 2.4. Day 4
