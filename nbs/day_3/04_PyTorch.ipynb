{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bsc-life/ml4br-ml-course/blob/main/nbs/day_3/04_PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejdOqrsEm340"
   },
   "source": [
    "# Basic Neural Network\n",
    "\n",
    "We are going to code the neural network we just saw in the slides. We will use PyTorch to do this. Confusingly enough, in python, PyTorch is just called `torch`. So, when you see `torch`, it is referring to the PyTorch library.\n",
    "\n",
    "`torch` has many submodules. The most important ones are:\n",
    "- `torch.nn`: this is the module that contains all the neural network functions and classes, like layers, activation functions, etc.\n",
    "- `torch.optim`: this is the module that contains all the optimization functions and classes, like SGD, Adam, etc.\n",
    "\n",
    "We are also going to need `torch.nn.functional`, which contains all the functions that are not classes, like activation functions, loss functions, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T07:56:06.372333Z",
     "start_time": "2025-05-13T07:56:06.368598Z"
    },
    "id": "93Qbvq2Wm6vz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pandas.tests.arrays.integer.test_repr import test_dtypes\n",
    "from torch.optim import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNUH8DGHXXA_"
   },
   "source": [
    "Networks in PyTorch are specified through Python classes. These are basically a collection of functions. We define a class with `class`, give it a name, and declare if there's another class from which it inherits some behavior, known as the parent class. We then specify which functions the class contains:\n",
    "\n",
    "* `__init__`: this is a mandatory function\n",
    "  * `self` referst to the class itself. Whenever we specify this within the class, we are telling the class that it has different parts, namely, the weights and the biases of the different neurons in our network.\n",
    "  * `super`: refers to the parent function. We need to initialize it after the class.\n",
    "* `forward`: which takes as arguments the class itself, and some input, i.e., the data we will train and test the model on. It is important to call it `forward` and not something else. Pytorch or, more specifically, `nn.Module` expects this function to perform backpropagation. When we call the model in our data, we will call the class like\n",
    "\n",
    "`model=BasicNN() # Initialize the model`,\n",
    "\n",
    "which initializes the class (i.e., it sets the parameters). While training and testing we can call the model we just initialized\n",
    "\n",
    "`model(data) # For training and/or testing`\n",
    "\n",
    "and this will automatically call the forward method and then perform backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "This is the most common way to work on PyTorch! It is a very flexible coduct because we can have one single code to train many different models on different datasets! Think of it just like a normal Python function: you can use it in many situations without the need to copy many lines of code. Besides, classes can inherit behavior from others, so you can add new classes with new features that also retain the behavior you already programmed elsewhere, reducing the number of lines and code you need to spend programming.\n",
    "\n",
    "\n",
    "*Note: This type of programming, with classes, is known as Object-Oriented Programming (OOP) and it is very common! Just check the source code of complex packages such as `sklearn` and you will see it is mostly classes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:17:41.905151Z",
     "start_time": "2025-05-12T08:17:41.899347Z"
    },
    "id": "-JtOSquMnT-z"
   },
   "outputs": [],
   "source": [
    "class BasicNN(nn.Module): # We tell the class its parent is nn.Module\n",
    "  # Initiate the parameters\n",
    "  def __init__(self): # Here we tell the class: \"Cogito, ergo sum\"\n",
    "    super().__init__() # Here we intialize the parent\n",
    "\n",
    "    self.w00=nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
    "    self.b00=nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
    "    self.w01=nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
    "\n",
    "    self.w10=nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
    "    self.b10=nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "    self.w11=nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
    "\n",
    "    self.final_bias=nn.Parameter(torch.tensor(-16.), requires_grad=False)\n",
    "  # Perform forward pass through the neural network\n",
    "  def forward(self,input):\n",
    "    input_to_top_relu = input*self.w00+self.b00\n",
    "    top_relu_output = F.relu(input_to_top_relu)\n",
    "    scaled_top_relu_output = top_relu_output*self.w01\n",
    "\n",
    "    input_to_bottom_relu = input*self.w10+self.b10\n",
    "    bottom_relu_output = F.relu(input_to_bottom_relu)\n",
    "    scaled_bottom_relu_output = bottom_relu_output*self.w11\n",
    "\n",
    "    input_to_final_relu = scaled_top_relu_output + scaled_bottom_relu_output + self.final_bias\n",
    "\n",
    "    output = F.relu(input_to_final_relu)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl4KE4QDfHVD"
   },
   "source": [
    "---\n",
    "## Using a Neural Network\n",
    "\n",
    "We create the same data we had in the example. Consider three groups of patients to which three different doses of a drug are given.\n",
    "* The first group receives no dose (0),\n",
    "* the second group receives an intermediate dose (0.5), and\n",
    "* the third group receives a high dose (1).\n",
    "\n",
    "The effectiveness of the drug is 0 for the first group, 1 for the second group, and 0 for the third group.\n",
    "\n",
    "To make things ever more simple, we consider just three patients, one per group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0lGxvSGfHVE"
   },
   "source": [
    "The process to apply our neural network is very simple. It is very similar to using `sklearn`:\n",
    "1. We initialize the model\n",
    "2. We train the model in our data\n",
    "3. We use the model to predict the effectiveness of the drug in a new patient.\n",
    "\n",
    "You can think about the previous model as being \"pre-trained\": it already contains the optimal parameters needed to predict the effectiveness of the drug in a new patient. In this case, we are not going to train the model, but we are going to use it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:17:42.125671Z",
     "start_time": "2025-05-12T08:17:41.975080Z"
    },
    "id": "IyEhZioinUci"
   },
   "outputs": [],
   "source": [
    "# Debug forward pass\n",
    "input_doses = torch.linspace(start=0,end=1,steps=11)\n",
    "model = BasicNN() # initialize the model\n",
    "output_values = model(input_doses) # run the forward pass\n",
    "\n",
    "plt.plot(input_doses.detach(),output_values.detach(),color='green',linewidth=2) # detach() is used to remove the gradient tracking, so no gradient will be backpropagated along this variable.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "plt.title('Effectiveness vs Dose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69rDhbItfHVG"
   },
   "source": [
    "---\n",
    "## Training a Neural Network\n",
    "It is not usually the case that we already know the optimal parameters of the model. In most cases, we need to train the model to find the optimal parameters. This is done by using a training set and a loss function.\n",
    "\n",
    "To optimize a parameter in PyTorch, we need to set the option `requires_grad=True` when we create the parameter. This tells PyTorch that we want to compute the gradient of this parameter with respect to the loss function. The loss function is the function that we want to minimize (the difference between the predicted value and the actual value).\n",
    "\n",
    "We are now going to optimize the final bias, i.e., the parameter that gets added to the output of the last layer. The rest of the parameters are going to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:17:42.153257Z",
     "start_time": "2025-05-12T08:17:42.147113Z"
    },
    "id": "JvFfj5_ZnVAS"
   },
   "outputs": [],
   "source": [
    "class BasicNN_train(nn.Module):\n",
    "  # Initiate the parameters\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.w00=nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
    "    self.b00=nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
    "    self.w01=nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
    "\n",
    "    self.w10=nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
    "    self.b10=nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "    self.w11=nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
    "\n",
    "    self.final_bias=nn.Parameter(torch.tensor(0.), requires_grad=True) # this parameter should be optimized\n",
    "  # Perform forward pass through the neural network\n",
    "  def forward(self,input):\n",
    "    input_to_top_relu = input*self.w00+self.b00\n",
    "    top_relu_output = F.relu(input_to_top_relu)\n",
    "    scaled_top_relu_output = top_relu_output*self.w01\n",
    "\n",
    "    input_to_bottom_relu = input*self.w10+self.b10\n",
    "    bottom_relu_output = F.relu(input_to_bottom_relu)\n",
    "    scaled_bottom_relu_output = bottom_relu_output*self.w11\n",
    "\n",
    "    input_to_final_relu = scaled_top_relu_output + scaled_bottom_relu_output + self.final_bias\n",
    "\n",
    "    output = F.relu(input_to_final_relu)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP_TyHyPfHVJ"
   },
   "source": [
    "Every time we initialize the model and then call it, it will default to the `forward` method defined within the class. This method is the one that performs the forward pass through the neural network. It takes the input and returns the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:17:42.372680Z",
     "start_time": "2025-05-12T08:17:42.214875Z"
    },
    "id": "TDMjSNjNq5Ph"
   },
   "outputs": [],
   "source": [
    "# Debug forward pass\n",
    "input_doses = torch.linspace(start=0,end=1,steps=11)\n",
    "model = BasicNN_train() # initialize the model\n",
    "output_values = model(input_doses) # run the forward pass\n",
    "\n",
    "plt.plot(input_doses.detach(),output_values.detach(),color='green',linewidth=2) # detach() is used to remove the gradient tracking, so no gradient will be backpropagated along this variable.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "plt.title('Effectiveness vs Dose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWQN9oBpfHVM"
   },
   "source": [
    "You can see this plot looks bad: The effectiveness goes all the way to 17! This is because the final bias is not optimized yet. The model is not able to predict the effectiveness of the drug in a new patient. We need to train the model to find the optimal final bias.\n",
    "\n",
    "Now, to train a neural network, we need to loop through a number of epochs. In each epoch, we need to:\n",
    "1. Get the input and the label (the actual value of the effectiveness of the drug in a new patient).\n",
    "2. Run the forward pass through the neural network to get the output.\n",
    "3. Compute the loss (the difference between the predicted value and the actual value).\n",
    "4. Backpropagate the loss to compute the gradients of the parameters with respect to the loss function.\n",
    "5. Update the parameters using the optimizer.\n",
    "\n",
    "\n",
    "The loss function we are going to use is the Mean Squared Error (MSE). This is the most common loss function used in regression problems. It is defined as the average of the squared differences between the predicted values and the actual values. The optimizer we are going to use is Stochastic Gradient Descent (SGD). This is the most common optimizer used in deep learning. It updates the parameters using the gradients computed during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:17:43.285376Z",
     "start_time": "2025-05-12T08:17:42.392539Z"
    },
    "id": "gVt4V_hZrHPe"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "inputs = torch.tensor([0.,0.5,1.])\n",
    "labels = torch.tensor([0.,1.,0.])\n",
    "\n",
    "# The optimizer we will use is SGD (Stochastic Gradient Descent)\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss\n",
    "losses = []\n",
    "# We train for 100 epochs\n",
    "for epoch in range(100):\n",
    "  total_loss = 0\n",
    "  # Loop through the training set\n",
    "  for iteration in range(len(inputs)):\n",
    "    # Get the input and the label\n",
    "    input_i = inputs[iteration]\n",
    "    label_i = labels[iteration]\n",
    "    # Apply the model to the input\n",
    "    output_i = model(input_i)\n",
    "    # Compute the loss\n",
    "    loss = criterion(output_i,label_i)\n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "    # Add the losses to get the accumulated loss for each epoch\n",
    "    total_loss += float(loss)\n",
    "\n",
    "  losses.append(float(total_loss))\n",
    "\n",
    "  if total_loss<0.0001:\n",
    "    print(f'Num steps: {epoch}')\n",
    "    break\n",
    "  # Update the parameters with the optimizer\n",
    "  optimizer.step()\n",
    "  # Zero the gradients for the next iteration (otherwise, they will accumulate)\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  print(f'Step: {epoch} Final Bias: {model.final_bias.data}\\n')\n",
    "  print(f'Loss: {total_loss}')\n",
    "\n",
    "print(f'Final Bias after Optimization: {model.final_bias.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7MyhGmEfHVN"
   },
   "source": [
    "It is usually a good practice to plot the loss of your model to test whether it is learning correctly.\n",
    "\n",
    "Has it learned the optimal final bias? The final bias should be around -16.0, which is the value we set in the first model. The loss should be around 0.0, which means that the model is able to predict the effectiveness of the drug in a new patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:17:43.458511Z",
     "start_time": "2025-05-12T08:17:43.302453Z"
    },
    "id": "zL2nxpObfHVO"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses,color='cornflowerblue',linewidth=2)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6lw32kxfHVQ"
   },
   "source": [
    "Now that we have obtained the optimal final bias, we can plot the effectiveness to get the optimal plot from the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:17:43.639399Z",
     "start_time": "2025-05-12T08:17:43.485502Z"
    },
    "id": "LlUKYBJNrHja"
   },
   "outputs": [],
   "source": [
    "output_values = model(input_doses) # run the forward pass\n",
    "\n",
    "plt.plot(input_doses,output_values.detach(),color='green',linewidth=2) # detach() is used to remove the gradient tracking, so no gradient will be backpropagated along this variable.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "plt.title('Effectiveness vs Dose')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odanzk7V5MoF"
   },
   "source": [
    "## An even simpler neural network\n",
    "\n",
    "The previous class `BasicNN` is not very flexible: Are you telling me I need to declare every weight, bias, activation layer and figure out how they are all connected? What if I need a very deep neural network with dozens of layers and hundreds of neurons???\n",
    "\n",
    "Well, fret not! There's a much simpler way with `torch`.\n",
    "\n",
    "Every hidden layer we just defined can easily be declared with `nn.Linear(in_features, out_features, bias=True)` with parameters:\n",
    "\n",
    "* `in_features`: the dimensions, or size, of our input data. In this case, that's just 0 (the dose variable has just one value per patient). *Tiny question*: what order of tensor is it??\n",
    "* `out_features`: the dimensions of the ouput we want. In other words, the number of neurons.\n",
    "\n",
    "As the activation layer, we are going to use `nn.ReLu`, just like before.\n",
    "\n",
    "We are going to wrap our neural network with two neurons and two activation functions around `nn.Sequential`, which allows you to easily combine many different layers.\n",
    "\n",
    "\n",
    "Just like we did before, we can initialize some of the parameters to make training easier. We are only going to train the final bias.\n",
    "\n",
    "Finally, the forward pass now is simply passing the input through the model we defined inside of `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T07:59:38.483105Z",
     "start_time": "2025-05-13T07:59:38.477411Z"
    },
    "id": "q-x60xQJekV6"
   },
   "outputs": [],
   "source": [
    "class BasicNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 2, bias=True),  # Input layer to 2 neurons\n",
    "            nn.ReLU(), # Hidden activation layer\n",
    "            nn.Linear(2, 1, bias=True), # Output layer with one neuron\n",
    "            nn.ReLU(), # Output activation layer\n",
    "        )\n",
    "        # Initialize parameters and freeze non-trainable ones\n",
    "        with torch.no_grad():\n",
    "            # First layer parameters (frozen)\n",
    "            self.model[0].weight = nn.Parameter(torch.tensor([[1.7], [12.6]]), requires_grad=False)\n",
    "            self.model[0].bias = nn.Parameter(torch.tensor([-0.85, 0.0]), requires_grad=False)\n",
    "            # Second layer paramters (frozen)\n",
    "            self.model[2].weight = nn.Parameter(torch.tensor([[-40.8, 2.7]]), requires_grad=False)\n",
    "            self.model[2].bias = nn.Parameter(torch.tensor([0.0]), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1) # Reshape data to [1,1]\n",
    "        return self.model(x).squeeze() # Ensure output has one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVFhbnUVsGT8"
   },
   "outputs": [],
   "source": [
    "# You should always doble check the shape of your tensors!\n",
    "print(torch.tensor(0).shape, torch.tensor([0]).shape, torch.tensor([0]).view(-1, 1).shape)\n",
    "print(torch.tensor([0]), torch.tensor([0]).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:00:37.197189Z",
     "start_time": "2025-05-13T08:00:37.044231Z"
    },
    "id": "mfS3mfx6ekV7"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "inputs = torch.tensor([0.,0.5,1.])\n",
    "labels = torch.tensor([0.,1.,0.])\n",
    "\n",
    "model = BasicNN()\n",
    "\n",
    "# The optimizer we will use is SGD (Stochastic Gradient Descent)\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss\n",
    "losses = []\n",
    "# We train for 100 epochs\n",
    "for epoch in range(100):\n",
    "  total_loss = 0\n",
    "  # Loop through the training set\n",
    "  for iteration in range(len(inputs)):\n",
    "    # Get the input and the label\n",
    "    input_i = inputs[iteration]\n",
    "    label_i = labels[iteration]\n",
    "    # Apply the model to the input\n",
    "    output_i = model(input_i)\n",
    "    # Compute the loss\n",
    "    loss = criterion(output_i,label_i)\n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "    # Add the losses to get the accumulated loss for each epoch\n",
    "    total_loss += float(loss)\n",
    "\n",
    "    losses.append(float(total_loss))\n",
    "\n",
    "  if total_loss<0.0001:\n",
    "    print(f'Num steps: {epoch}')\n",
    "    break\n",
    "  # Update the parameters with the optimizer\n",
    "  optimizer.step()\n",
    "  # Zero the gradients for the next iteration (otherwise, they will accumulate)\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  print(f'Step: {epoch} Final Bias: {model.model[2].bias}\\n')\n",
    "  print(f'Loss: {total_loss}')\n",
    "\n",
    "print(f'Final Bias after Optimization: {model.model[2].bias}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:00:45.621514Z",
     "start_time": "2025-05-13T08:00:45.419320Z"
    },
    "id": "_83StnunekV7"
   },
   "outputs": [],
   "source": [
    "# Debug forward pass\n",
    "input_doses = torch.linspace(start=0,end=1,steps=11)\n",
    "output_values = model(input_doses) # run the forward pass\n",
    "\n",
    "plt.plot(input_doses.detach(),output_values.detach(),color='green',linewidth=2) # detach() is used to remove the gradient tracking, so no gradient will be backpropagated along this variable.\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.xlabel('Dose')\n",
    "plt.title('Effectiveness vs Dose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JKuuwcUfHVU"
   },
   "source": [
    "# Binary Classification of a Table\n",
    "\n",
    "Usually we do not have such simple data. In most cases, we have a lot of data and we need to train the model to find ALL the optimal parameters. In this case, we are going to use a dataset from the UCI Machine Learning Repository. This dataset contains information about breast cancer patients and whether they have benign or malignant tumors.\n",
    "\n",
    "Neural Networks can be used to solve any of the problems you have seen so far in the course. In this case, we are going to use a neural network to solve a binary classification problem. The goal is to predict whether a tumor is benign or malignant based on the features of the tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWC305Xl2fEO"
   },
   "outputs": [],
   "source": [
    "! pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:32.515018Z",
     "start_time": "2025-05-12T09:35:30.935244Z"
    },
    "id": "FzrezysDfHVU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\n",
    "# Adjust the data\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()[:, 0]  # Convert targets to a 1D array\n",
    "# Map 'B' to 0 and 'M' to 1\n",
    "y = np.where(y == 'B', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESGB1Nr-ekV9"
   },
   "source": [
    "It is very important in deep learning to normalize the data. This is because the neural network is very sensitive to the scale of the data. If the data is not normalized, the neural network may not be able to learn properly. In this case, we are going to use the `StandardScaler` from `sklearn` to normalize the data so that it has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:33.308017Z",
     "start_time": "2025-05-12T09:35:33.302744Z"
    },
    "id": "aSj5V81zfHVV"
   },
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9i7VDFWekV9"
   },
   "source": [
    "Just like in the previous day, we split the data into a training and a test set. This time we need to convert them to tensors so that we can use them in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:34.925006Z",
     "start_time": "2025-05-12T09:35:34.918302Z"
    },
    "id": "dIzTuF0lfHVV"
   },
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:35.822081Z",
     "start_time": "2025-05-12T09:35:35.817819Z"
    },
    "id": "geD3aitJfHVV"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Add dimension\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5Y5GLsQekV9"
   },
   "source": [
    "The following class contains the neural network for a binary classifier.\n",
    "The neural network has three layers:\n",
    "1. The first layer has 16 neurons and uses the ReLU activation function.\n",
    "2. The second layer has 8 neurons and also uses the ReLU activation function.\n",
    "3. The last layer has 1 neuron and uses the Sigmoid activation function. Setting the final value of the network as one means we will get one prediction for each input. The Sigmoid function will then output a value between 0 and 1, which can be interpreted as a probability of the tumor being malignant.\n",
    "\n",
    "This process is wrapped in the `nn.Sequential` function, which allows us to define the model in a more compact way. The nn.Sequential function takes a list of layers and creates a model that applies each layer in sequence.\n",
    "\n",
    "The `input_size` takes the number of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:37.163930Z",
     "start_time": "2025-05-12T09:35:37.158367Z"
    },
    "id": "t_DGkmtDfHVW"
   },
   "outputs": [],
   "source": [
    "class BreastCancerClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BreastCancerClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()  # Sigmoid for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmnBBgcWekV-"
   },
   "source": [
    "Just like we did in the previous example, we need to define the loss function and the optimizer. In this case, we are going to use the Binary Cross-Entropy Loss (BCE) as the loss function. This is the most common loss function used in binary classification problems. The optimizer we are going to use is Adam, one of the most common optimizers used in deep learning. It implements an adaptive learning rate optimizer that is very efficient and works well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:39.436718Z",
     "start_time": "2025-05-12T09:35:39.430550Z"
    },
    "id": "hDbfBL-OfHVW"
   },
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1] # number of features\n",
    "model = BreastCancerClassifier(input_size) # Initialize the model\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss; used for classification\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zYerZMrekV-"
   },
   "source": [
    "In this case we are going to simultenously train and then test the model. This is a good practice in order to check for overfitting. After training is done, we should see that the training loss is lower than the testing loss by a small margin. This means the model has learned to generalize from the training data to data it has never seen before. If the testing loss was much higher than the training loss, it would mean that the model has overfitted to the training data and is not able to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:45.266297Z",
     "start_time": "2025-05-12T09:35:43.461114Z"
    },
    "id": "xYyLn7zRekV-"
   },
   "outputs": [],
   "source": [
    "# Train and test the model\n",
    "epochs = 1_000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    train_loss = criterion(outputs, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:35:51.429359Z",
     "start_time": "2025-05-12T09:35:51.275674Z"
    },
    "id": "YnHUmKhIekV_"
   },
   "outputs": [],
   "source": [
    "# Plot training and testing losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(test_losses, label='Testing Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3tl9ZXcekV_"
   },
   "source": [
    "Once the model is trained, we can assess it on the test data. To do so with PyTorch, we need to set the model to evaluation mode and use the `torch.no_grad()` context manager to disable gradient calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:36:11.891429Z",
     "start_time": "2025-05-12T09:36:11.884325Z"
    },
    "id": "qt7AZPXwfHVX"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval() # Set the model to evaluation mode\n",
    "with torch.no_grad(): # No need to compute gradients during evaluation\n",
    "    predictions = model(X_test)\n",
    "    predictions = (predictions > 0.5).float()  # Convert probabilities to binary predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFq07EQwekV_"
   },
   "source": [
    "Now that we have the predictions, we can compute some metrics to evaluate the model with `classification_report`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:36:16.994230Z",
     "start_time": "2025-05-12T09:36:16.970162Z"
    },
    "id": "XxGVngTxfHVY"
   },
   "outputs": [],
   "source": [
    "# Classification report:\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = predictions.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "print(classification_report(y_test_np, y_pred, target_names=['Benign', 'Malignant']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glXdNByVekWE"
   },
   "source": [
    "![hacker.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAABuwAAAbsBOuzj4gAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAA2oSURBVHic3Zt7cNTXdcc/57e/32q1eu4KPZBAgBVJyMgPgmyDwUQQgh886sgPsIvHJk6czNS1U9MMk6QNZOKkdg2ePjJ1MrhJ7cStgmmCHxTixHbsMsZ2sbGNZQUbjCkIAQI9QLur1e7+Tv9YrazVrtCutKJuvzO/0U/3d+6555x77r3nnntXVJULBRERYCVwA1AHXAyEgX3AO8B/Ac+oauSCCaWqE/4AAtyIyHuAxh4xnSoOU4eWIcYfgWUXQi5VRSbaA0SkGvgVMBsEV3k1ubVzsYrKsR1CuN9HuKeD8Jl2QieP0H/icKzqy8B9qvr+hMo3kQYQkQZgJzDJzCtiUuManEVT4mjCIT/BYDcMyBE69d/49r1MpLcLoBe4SVVfmDAZJ8oAIrIUMbajdrZ7+qV459+CYbmS0tp2iGCgE9sOA6B2hMAf3yTQ+gaIRFD9iqo+OSFyToQBROR2kCfEMMzCK1aQVzd/1DpqRwj4Oxg6/wWPtNL71gsx77hfVf8h47Jm2gAi0gRsQwwp/uJasqfMTLmuHQkRCHQMDgeA/raDnHtzJ9gRG1ipqjsyKm8mDSAilYi8h2qBd14TubXz0uYRCffRFzgTV9bf/jHn9jwH4EP1SlX9IDMSg5EpRiLiAGlGtSC/vnFMygM4TBdOV2FcmXPyRbjr54NqDsgOESnKhMyQQQMA3wed555+KYUNN4yLkWXlYDlz48qyaxrIqqwDdDpw77gaGAIzE0xEZBHwHTPXg3f+rUTjnvHBmVWAHQkRiQQHy3LmLMH0lpHlneoZdwMDGPccICKTENmPUlZy3ddxlVVlSLTo8hjwnUrSpkOdpk5r+7fNR8fbRiaGwE9RLcu7eEFGlQcwDCthKACoRsR/vO19EfnquNsYT+WBSK/JzPVQOOf68cqSFJYzHxFHQrnvwOv5wBYRWTge/uP1gI0A+Zd+EXFY42SVHCKC01WQUO6aPiv2umE8/MdsgIHeX2bmesn5XMN4ZBgVppmNw5EVV+acUoPpKQVYLCKjh5ojYDwesBEg/9LFiJHoopnG8NgAiMYGUXxvrHzHZIAL2fsxGIaJw4zfTFkllVgllQBLReSqMfEdozwb4cL1fgyWMy+hzF2/IPY6Ji9IOxBKt/c1HCLs6yISOIftP0skcC76Hg5+uukRwXC6sfKKMPOLMPMm4XDnJ/ByOJw4HE4ikf5PFfCU4Jw8g/72wzeISIOq7k1Hn7FEghshee9rJEz/mWPR53T0b6inA9ROuxHDdGLmFUWf/EmY+UVYBSWYntI4AwC4qi6nv/0wRL1gZTrtpGUAEfk8w3pfI2H62g7g++RdAkc/QEPB89UnJ9tFeamXtbcs5eLqaQB88NERfv70Cxw/2Ykv0IeqYof76e9qp7+rPY6H4czGOXkGVnkVVuk0xGFilU7Dkesh0tu1QkRmq+q+lJVKM7n5EKDeuU1asep7mlc3Xw3LFZfUXL74Kv2Lu5viE51Jnhy3Szvf2aad72zTHLdrVPp719yg1y+cHVcmpqWuz81Wz/J7NOeyxlj5pnR0SncIXAdCf2cbXXufQ8MhXFkWK69t4OU3D9DRdY6ZVVM5eOQ4AGVlZZSVlSUwOXHiBCdOnOCaW9cB4PP3jUp7pO0UNdMr2PnqPoo9eSy6spbtL79L38F9BA/vx1k5E0RA9TrgL1NVKOXNkIiUA21DCkCVP112JT/96zXMu+Nh9n/UFlenpqaG2traBF4HDhzgww8/HDPtJdUV7PnFer7+g1/y1I43B2UZgqmqeiwVvdJZBq+NvVjFU8hf0ATA261HsW2lvDgarv7Vvbdx96prk3MAbNvm9OnTADz7+EaefXwjAKdPn8a2R54s77yxkfVf+zIA5cUF2Lbydmt0M5i/oAmrOC7bPLIAw5HG+N8KqGtGvRY13a9FN31TTU+pAlpRUqjlxQUK6E9+eJ/uffYfFVDLsjQ7OzvusSxLAV0yf7bq4V2qh3fpkvmzR6V/9akH9e+++xUFtLy4QCtKChVQ01OqRTd9U4ua7lfXjPrYPPB0qnql4wGLMRy4L7km6nJA3tzlWMVTaDvVzfGOHgD++Ve7mHNJNeu/cStul5NAIBD3eAty+NY9N/OzRx4YZPyzRx7gW/fcjLcgJ4E+O8vigbUr+PzFF7F152sAHO/ooe1UN1bxFPLmLo8yEYnKFl2aF6WqVDpzwB5gbv41TbHwcxDa34cd9HNu92+I+M/x4lMPsfjqy1OVIQH+s93osOHwwcGjXHnLehzuPPIWfBkjy40440Pj0MkjnN39G4DXVTWlpGQ6HvAMgL9lT8IHcbpw5HlxVc8B4KHHtqbBNh6hYF+C8gCP/stzALiq5+DI8yYoD+D/YFC2Z1JtLx0D/Bg4Ge5sJ3Tik6QEWTPqMVw5/G732zz34utpsI5CVQn1BRLKt/12D807dmNkucmaUZ+0bn/7YcKdJwBODMiaElI2gKr2At8F8Le8Bkl6SRwmuQ1LAbjlz37ES6+9kyp7APoDfoYPyQ8/Oc6f/+BxAHKvWIo4EkMXjYTxv7879u+3B2RNCenuBn8O/CHcfQrfe68kJbBKp5Fd20Aw2M/Kr27ktbdSO8MI+n2E++PD6D37DrDkro2c8wXIrpmDVTo9aV3fvpeInD0D8DvgiVSVgTQNoKo2cBtwsu/QuwSPJFfOXb8AV9Xl+AJ9LFnzbb7/97+k15/o2jEEfb1xyofCEX7S/FuWfeNHdPb04qq6bOi2Nw5D5GgDbtc009xjSouLSCPwezEcRn7jKjE9JUnpAq1v4G99HVQpmVTIhvvWsPaWpWS7nABEwiFCfQEi4eipcLA/xPbfv8mDj23j8LGT0aVt1tVk116RlH/4zHF6XtkGaoeBa1Q17YlnzOcCIvId4IeGO4+CxbdjZGUnF7KnA9++lwifie7qDEOoLC+hdkY5tdPL8Rbm0tndS8vBo7zx3kf0+qKeYhVPIeeyRhwFk5Lytft89Lz4r2r3+YRxnByPxwACPA/cYBVPJf+apsEAaTg0EqZz+4+xTBPLcuAPJN8yOy2T/lAY0zuZgkWrRm7ctul59emYUbeq6nmIz48xH42pqorIHcC+UMfRSt/+/yTn0uQp+nBntPcvq7uI1379KO+2fszBT9o53RWNHj35ucysmkplRQmlDauJnDuD3efDcOUk5ed79w8x5Q8A4zocSdsDRKQAuBO4CcgCKoApQDRNncQLbP9Z7D4/3sI8qqdXnJf/3v0fEonYONz5iMudSKBKuOtk7L9PgJNAEPh34AlV7UlLn3QMICK5InJUVRNz1J8BiEi3qk5NJw5IdwisVtXCFStWcPfdd2NZE3MadD5EIjZ9wficYCgc4hdPPsmunf9RCKwGHk+VX7oe8BywfOvWrVRXV6dcL9Po9QcYLvbHhw5x1513ADyvqitS5ZVuJGgB/ys9H4/EecY0B505LeFSNsDAKey4TmIvCEQWikjysDEJUjKAiMxDZCeQPNr5LEE1GzF2icicVMhHNYCIXIHIC4A72WnNZw3R2EFzMIwXRWTWaPTnXQVEZCrwAqq5nqtuJHCslYj/bEYEbWlpYe/e6ClWQ0MDs2aNKmtKcBRMIstbRqD1jQJEXhKRear68Uj0oy2DDwKFubXzyKubT+BYa0aEfPjhh2lubo4rW716NevXr88If3fdXMKn2wh1HCsBHgVuHIl2xCEgIpcDa6zCUrxXjVg/bezYsYPm5mY8Hg/r1q1j3bp1eDwempub2bEjQ5dARci98nqMLDfAnwwc6SXF+eaAvwWMgsuXgpG564TPP/88AFu2bGHTpk1s2rSJLVu2xH3LBAxXDjkNX4r9O+I1mqSaichS4EuWZzLu6ZdkTCiA9vboxmjhwk9X1Nh77Fum4CybEdtOrxxpVRipa78GUDB7KZm49DgUNTU1ADz22GODZbH32LdMwlU1mJ7fmOx7UgNkVc560nv1zbgrk2dgx4O1a9diGAYbNmygvr6e+vp6NmzYgGEYrF27NuPtZVXORJxZAMuTLYtJDVC66K4ZuTVjunIzKurq6ti8eTNFRUW0tLTQ0tJCUVERmzdvpq6uLuPticPENW2wIxMuTyQsgyLimnbXI00T+UuixsZGFi5cyKFDhwCoqqrCyOBEOxxZ0+oIfPQWwDLgb4Z+i2s1egQuHd1v75rwmN8wDKqrq6murs6o8iKJvBwFk2LDYK6IeOPkGEb7BdBcOxTM7Mx3AZHMAACWtxzAwbCj8+HUDQDOSVP4vwoZITFrFk2OvcZdah4+BwwYYGoiB9uGSDR/39XVRV5e4p29CwWfvw8lfpbq7ukGQG2N3kob5glmUXnsNe5u32BGSKK+c1YsZ87U2x+MS26ebXmFcy2vZmwjNNFwZOeSVT2b7OpPYx+NhOl85p+iP0+DHFUNQrwHzARynN6KOOXDvV10792BZVpMuyizvwcYL1SVSJJD2vbjbfj37yarohpjYAsvDhOzsJhw10kH0d8s74N4A9QAOL3xaevAsVZQpXHRIlatum1iNBkjwuEwviRnjs9s/zWvvvIy/e2HcVVdNlhuestiKfXpDBhg6BCYDLSJw8KRnTvoAnYoiB30YxgGHk/GfqqTESgDY34Yenq6sW0bcboQ0/kpfdCPRsIKVKhqdOMx7CLUPcChGO//f48cAu4ZqvP/AEDejvumJqsBAAAAAElFTkSuQmCC)\n",
    "\n",
    "**Exercise**. There's a rather obvious disadvantage in our `BreastCancerClassifier` network. It predicts just one value for each patient. This is alright for a binary classification, but is unsuitable for multi-class classification. We can create a more general class that can be used for binary or multi-class classification. Go back to the `BreastCancerClassifier` class and modify it to accept an arbitrary number of classes. *Hint*: you only need to change two lines in total: one to update the number of classes, and another one to adjust the output. Remember, how does the data look like after the final activation layer and how does it look like before?\n",
    "\n",
    "In the training loop, instead of using `nn.BCELoss()`, you should use `nn.CrossEntropyLoss()` for multi-class classification with the correct format.\n",
    "\n",
    "You will also need to format again the target labels:\n",
    "``` python\n",
    "# Adjust the target labels for CrossEntropyLoss\n",
    "y_train = y_train.squeeze().long()  # Convert to long for CrossEntropyLoss\n",
    "y_test = y_test.squeeze().long()\n",
    "```\n",
    "\n",
    "Plus, the output of the model is now a matrix instead of a vector. After getting the predictions, you can set `predictions = torch.argmax(predictions, dim=1)` to get the predicted class indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpc4FsppekWJ"
   },
   "source": [
    "# Image Classification (CIFAR10)\n",
    "\n",
    "As we discussed in class, we can use a Convolutional Neural Network (CNN) to process images. In this section we are going to learn how to create a CNN with PyTorch and we are going to use it to classify different images, i.e, distinguishing the subjets of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:14:39.970926Z",
     "start_time": "2025-05-13T08:14:39.729977Z"
    },
    "id": "Pm93jfzGekWJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:15:56.727367Z",
     "start_time": "2025-05-13T08:14:54.198702Z"
    },
    "id": "XbnvwOtHekWJ"
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:15:57.089667Z",
     "start_time": "2025-05-13T08:15:56.803935Z"
    },
    "id": "QYq4Zy5eekWO"
   },
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw7wXlBYvCtA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:23:13.189207Z",
     "start_time": "2025-05-13T08:23:13.180502Z"
    },
    "id": "5iviz3nzekWT"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Output size formula: 1+(Input Size−Kernel Size+2×Padding)/Stride\n",
    "            # Stride is 1 unless specified otherwise\n",
    "            # Input is (3,32,32)\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5), # (6, 28, 28); i.e. (32-5)/1+1 = 28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Stride is the jump necessary to go from one element to the next one in the specified dimension dim\n",
    "            # Output is (6,14,14), i.e. (28-2)/2+1=14\n",
    "            nn.Conv2d(6, 16, 5), # Output: (16, 10, 10), i.e., (14-5)+1=10\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # Output: (16, 5, 5), i.e., (10-2)/2+1=5\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 5 * 5, 120), # Input is the size of the last layer flattened\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10) # Output layer has size equals to the number of classes to predict\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) # First network: Convolutional\n",
    "        x = self.classifier(x) # Second network: Classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsLaOgr_DjBp"
   },
   "source": [
    "CNNs are a lot more complex than the basic neural networks we coded for processing tabular data. For this reason, it is recommended that we use a GPU to train our CNN. It is important to declare which device we are using, since tensors in the CPU and GPU cannot be operated together. We must either sum, multiply, etc. tensors in the GPU OR in the CPU. To do so, we resource to the method `.to()` which is available both in tensors and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLGf1mbG1Tml"
   },
   "outputs": [],
   "source": [
    "# Initialize model and move to GPU\n",
    "net = Net().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:23:14.253047Z",
     "start_time": "2025-05-13T08:23:14.247348Z"
    },
    "id": "MJwZt6lyekWT"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:26:27.174897Z",
     "start_time": "2025-05-13T08:23:15.397829Z"
    },
    "id": "XGa0cNdvekWT"
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3dcieIGEQSD"
   },
   "source": [
    "To test the model, we again need to pass the test data to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:19:32.630506Z",
     "start_time": "2025-05-13T08:19:20.796600Z"
    },
    "id": "gmfXe7x4ekWZ"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.eval()  # Set model to evaluation mode\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to('cuda')).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10_000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T08:19:44.360297Z",
     "start_time": "2025-05-13T08:19:32.709987Z"
    },
    "id": "Qn11K_TbekWa"
   },
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttMmQ-1bekWa"
   },
   "source": [
    "# Image Classification (Breast)\n",
    "\n",
    "The previous example was a very simple one: the images were small (you could say, very pixelated), and the CNN could hardly tell them appart. Besides, it is not a very interesting case for the biologists in the room.\n",
    "\n",
    "In the next example we are going to use a dataset of histological images of breast tissue samples, some of which show Invasive Ductal Carcinoma (IDC), a type of breast cancer (see details in [Kaggle](https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images) or the [paper](https://doi.org/10.1117/12.2043872)).\n",
    "\n",
    "The images are bigger and more complex. So our CNN is also going to be bigger and more complex.\n",
    "\n",
    "*Note: you should really be using a GPU now! Otherwise, expect ~45 minutes of model training on a basic CPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:04:10.471373Z",
     "start_time": "2025-05-12T14:01:12.883785Z"
    },
    "id": "hhCCgIbFekWh"
   },
   "outputs": [],
   "source": [
    "# Download the data\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"paultimothymooney/breast-histopathology-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:50:43.700051Z",
     "start_time": "2025-05-13T09:50:35.415998Z"
    },
    "id": "uziARW0vekWi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import keras.utils as image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:50:43.717161Z",
     "start_time": "2025-05-13T09:50:43.711717Z"
    },
    "id": "ZldH4UfRekWi"
   },
   "outputs": [],
   "source": [
    "# We set our device to the GPU:\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:50:46.267574Z",
     "start_time": "2025-05-13T09:50:43.859550Z"
    },
    "id": "hxlA9L5PekWi"
   },
   "outputs": [],
   "source": [
    "# Look for all the available images\n",
    "# About 2 minutes to run\n",
    "breast_img = glob.glob('/kaggle/input/breast-histopathology-images/IDC_regular_ps50_idx5/**/*.png', recursive = True)\n",
    "\n",
    "for imgname in breast_img[:3]:\n",
    "    print(imgname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:50:47.104479Z",
     "start_time": "2025-05-13T09:50:46.285295Z"
    },
    "id": "DIYSDvwbekWj"
   },
   "outputs": [],
   "source": [
    "# Here we build two lists:\n",
    "N_IDC = [] # samples with no cancer\n",
    "P_IDC = [] # cancer patient samples\n",
    "\n",
    "for img in breast_img:\n",
    "    if img[-5] == '0' :\n",
    "        N_IDC.append(img)\n",
    "\n",
    "    elif img[-5] == '1' :\n",
    "        P_IDC.append(img)\n",
    "plt.figure(figsize = (15, 15))\n",
    "\n",
    "some_non = np.random.randint(0, len(N_IDC), 6)\n",
    "some_can = np.random.randint(0, len(P_IDC), 6)\n",
    "\n",
    "s = 0\n",
    "for num in some_non:\n",
    "\n",
    "        img = image.load_img((N_IDC[num]), target_size=(100, 100))\n",
    "        img = image.img_to_array(img)\n",
    "\n",
    "        plt.subplot(6, 6, 2*s+1)\n",
    "        plt.axis('off')\n",
    "        plt.title('no cancer')\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        s += 1\n",
    "s = 1\n",
    "for num in some_can:\n",
    "\n",
    "        img = image.load_img((P_IDC[num]), target_size=(100, 100))\n",
    "        img = image.img_to_array(img)\n",
    "\n",
    "        plt.subplot(6, 6, 2*s)\n",
    "        plt.axis('off')\n",
    "        plt.title('IDC (+)')\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:51:44.525828Z",
     "start_time": "2025-05-13T09:50:49.359955Z"
    },
    "id": "tdoLG4xPekWj"
   },
   "outputs": [],
   "source": [
    "# Now we load the available images into memory\n",
    "\n",
    "non_img_arr = [] # non cancer images\n",
    "can_img_arr = [] # cancer images\n",
    "# We will not load all of the images because it takes very long!\n",
    "# for img in N_IDC[:78786]:\n",
    "for img in N_IDC[:1_000]:\n",
    "    n_img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    n_img_size = cv2.resize(n_img, (50, 50), interpolation = cv2.INTER_LINEAR)\n",
    "    non_img_arr.append([n_img_size, 0])\n",
    "\n",
    "for img in P_IDC[:1_000]:\n",
    "    c_img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    c_img_size = cv2.resize(c_img, (50, 50), interpolation = cv2.INTER_LINEAR)\n",
    "    can_img_arr.append([c_img_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:51:45.202778Z",
     "start_time": "2025-05-13T09:51:44.544166Z"
    },
    "id": "89sOsINMekWn"
   },
   "outputs": [],
   "source": [
    "# Combine the two lists\n",
    "breast_img_arr = non_img_arr + can_img_arr\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(breast_img_arr)\n",
    "\n",
    "# Separate features and labels\n",
    "X = [feature for feature, label in breast_img_arr]\n",
    "y = [label for feature, label in breast_img_arr]\n",
    "\n",
    "# Convert to NumPy arrays; this is for scaling and data splitting\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:51:46.735249Z",
     "start_time": "2025-05-13T09:51:45.223112Z"
    },
    "id": "AEWEHsgxekWp"
   },
   "outputs": [],
   "source": [
    "# MinMax Scale the data:\n",
    "# sklearn takes too long so instead:\n",
    "X_scaled = X.astype('float32')\n",
    "X_scaled /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:51:56.984643Z",
     "start_time": "2025-05-13T09:51:56.741575Z"
    },
    "id": "-JuM6vlsekWq"
   },
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y,train_size=0.8, test_size=0.2,stratify=y)\n",
    "\n",
    "# If you manage to load all of the images, you can split the data while also\n",
    "# sub-sampling, i.e., selecting less samples than available:\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, y,train_size=0.2, test_size=0.05,stratify=y)\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSlB8ucDI67p"
   },
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "num_classes = 2\n",
    "Y_train = F.one_hot(torch.tensor(Y_train), num_classes=num_classes)\n",
    "Y_test = F.one_hot(torch.tensor(Y_test), num_classes=num_classes)\n",
    "print(\"Training Labels Shape:\",Y_train.shape)\n",
    "print(\"Testing Labels Shape:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:51:58.766258Z",
     "start_time": "2025-05-13T09:51:58.759400Z"
    },
    "id": "DZQdrwbxekWq"
   },
   "outputs": [],
   "source": [
    "# This is our BIG CNN\n",
    "class TumorClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # input: (3, 50, 50)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # output: (32, 25, 25)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),  # output: (64, 12, 12)\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 12 * 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:52:00.336871Z",
     "start_time": "2025-05-13T09:52:00.160121Z"
    },
    "id": "7ccxRYPhekWr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert data to PyTorch tensors if not already\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)  # For image data (N, C, H, W)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:52:01.892869Z",
     "start_time": "2025-05-13T09:52:01.877432Z"
    },
    "id": "NvFT8rO2ekWt"
   },
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(\"Batch of images shape:\", images.shape)\n",
    "    print(\"Batch of labels shape:\", labels.shape)\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T09:52:03.524083Z",
     "start_time": "2025-05-13T09:52:03.277114Z"
    },
    "id": "QZP0avv_ekWu"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 3, 1, 2)  # (N, C, H, W)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long).argmax(dim=1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long).argmax(dim=1)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:03:47.641501Z",
     "start_time": "2025-05-13T09:52:04.829886Z"
    },
    "id": "yNuPhOBLekWv"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Initialize model, loss, and optimizer\n",
    "model = TumorClassifierCNN()\n",
    "model.to(device) # Move model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:11:34.581826Z",
     "start_time": "2025-05-13T10:11:22.190860Z"
    },
    "id": "kHZTIHmFekWv"
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Concatenate all predictions and labels\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:11:35.221097Z",
     "start_time": "2025-05-13T10:11:35.121654Z"
    },
    "id": "yg_pQ-j3ekWw"
   },
   "outputs": [],
   "source": [
    "# Classification report:\n",
    "from sklearn.metrics import classification_report\n",
    "# y_pred = predictions.numpy()\n",
    "# y_test_np = y_test.numpy()\n",
    "print(classification_report(all_preds, all_labels, target_names=['Benign', 'Malignant']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
